# low_reward_thres: 0.1 -> 0.25
# eoe_margin: 1.0 -> 0.0
# reward weight edited
# run: PPO -> APPO

# DEFAULT_CONFIG = with_base_config(impala.DEFAULT_CONFIG, {
#     # Whether to use V-trace weighted advantages. If false, PPO GAE advantages
#     # will be used instead.
#     "vtrace": False,

#     # == These two options only apply if vtrace: False ==
#     # Should use a critic as a baseline (otherwise don't use value baseline;
#     # required for using GAE).
#     "use_critic": True,
#     # If true, use the Generalized Advantage Estimator (GAE)
#     # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
#     "use_gae": True,
#     # GAE(lambda) parameter
#     "lambda": 1.0,

#     # == PPO surrogate loss options ==
#     "clip_param": 0.4,

#     # == PPO KL Loss options ==
#     "use_kl_loss": False,
#     "kl_coeff": 1.0,
#     "kl_target": 0.01,

#     # == IMPALA optimizer params (see documentation in impala.py) ==
#     "rollout_fragment_length": 50,
#     "train_batch_size": 500,
#     "min_iter_time_s": 10,
#     "num_workers": 2,
#     "num_gpus": 0,
#     "num_data_loader_buffers": 1,
#     "minibatch_buffer_size": 1,
#     "num_sgd_iter": 1,
#     "replay_proportion": 0.0,
#     "replay_buffer_num_slots": 100,
#     "learner_queue_size": 16,
#     "learner_queue_timeout": 300,
#     "max_sample_requests_in_flight_per_worker": 2,
#     "broadcast_interval": 1,
#     "grad_clip": 40.0,
#     "opt_type": "adam",
#     "lr": 0.0005,
#     "lr_schedule": None,
#     "decay": 0.99,
#     "momentum": 0.0,
#     "epsilon": 0.1,
#     "vf_loss_coeff": 0.5,
#     "entropy_coeff": 0.01,
#     "entropy_coeff_schedule": None,
# })

run: APPO
name: exp21
local_dir: /checkpoint/jungdam/Research/ScaDive/data/learning/imitation/
checkpoint_freq: 500
checkpoint_at_end: true
stop: 
    time_total_s: 240000
config:
    env: HumanoidImitation
    gamma: 0.95
    lambda: 0.95
    clip_param: 0.2
    num_sgd_iter: 10
    lr: 0.00001
    horizon: 300
    train_batch_size: 256
    rollout_fragment_length: 128
    max_sample_requests_in_flight_per_worker: 1
    entropy_coeff: 0.0
    epsilon: 0.00001
    model:
        fcnet_activation: relu
        fcnet_hiddens: [256, 256]
        free_log_std: true
        vf_share_layers: false
    num_workers: 64
    num_gpus: 0
    batch_mode: truncate_episodes
    observation_filter: MeanStdFilter
    env_config:
        project_dir: /private/home/jungdam/Research/ScaDive/
        # Simulation
        fps_sim: 480
        fps_con: 30
        self_collision: true
        add_noise: false
        verbose: false
        sim_window: 120
        eoe_margin: 0.0
        # 'none', 'spd', 'pd', 'cpd', 'cp', 'v
        actuation: spd
        # 'imitation', 'heading', 'carry', 'dribble', 'fight', 'chase'
        task: 
            - imitation
        state:
            # 'body', 'imitation', 'interaction', 'task'
            choices:
                - body
                - task
        action:
            # 'absolute', 'relative'
            type: "absolute"
            range_min: -3.0
            range_max: 3.0
            range_min_pol: -15.0
            range_max_pol: 15.0
        reward: 
            # 'pose', 'vel', 'ee', 'root', 'com', 'interaction'
            choices: []
            fn_def:
                default:
                    name: total
                    op: mul
                    child_nodes:
                      - name: pose_pos
                        op: leaf
                        weight: 1.0
                        kernel: 
                            type: gaussian
                            scale: 40.0
                      - name: pose_vel
                        op: leaf
                        weight: 1.0
                        kernel: 
                            type: gaussian
                            scale: 2.0
                      - name: ee
                        op: leaf
                        weight: 1.0
                        kernel: 
                            type: gaussian
                            scale: 10.0
                      - name: root
                        op: leaf
                        weight: 1.0
                        kernel: 
                            type: gaussian
                            scale: 5.0
                      - name: com
                        op: leaf
                        weight: 1.0
                        kernel: 
                            type: gaussian
                            scale: 5.0
            fn_map:
                - default
        early_term:
            # 'task_complete', 'falldown', 'root_fail', 'low_reward'
            choices:
                - task_complete
                - low_reward
            low_reward_thres: 0.25
            falldown_contactable_body:
                - lankle
                - lknee
                - rankle
                - rknee
        character:
            char_info_module:
                - amass_char_info.py
            sim_char_file:
                - data/character/amass.urdf
            ref_motion_scale:
                - 1.0
            base_motion_file:
                - data/motion/amass/amass_hierarchy.bvh
            ref_motion_file:
                # - data/motion/amass/amass_hierarchy.bvh
                - data/motion/amass/CMU/85/85_14_poses.bvh
            environment_file: []